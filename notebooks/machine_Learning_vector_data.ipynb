{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamlidberg/Geographical-Intelligence-Lab/blob/main/notebooks/machine_Learning_vector_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48d6zNMNnNuZ"
      },
      "source": [
        "# Machine learning on vector data\n",
        "The core idea of machine learning is to program a system using data instead of with code. There are multiple types of machine learning which are used for different types of data such as images, text or numbers, but there are two main subdomains: traditional machine learning and deep learning. Machine learning was introduced around 1940 and deep learning 1963 but it was mainly deep learning that exploded in the 2000s. Deep learning is great but it is not always the best solution, especially when working with tablular data (tables with rows and columns). Therefore, this module will introduce you to traditional machine learning on geopandas dataframes.\n",
        "\n",
        "## Random forest\n",
        "Random forests is a very robust machine learning method which I both love and hate. I love it because it is so easy to use and always provide a good baseline. I hate it because it is hard to come up with something more powerful and novel.\n",
        "\n",
        "Random forest can be used for both classification and regression and works by building many decicion trees on randomly selected parts of your dataframe. Multiple deicion trees makes a decision forest hence the name random forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9ipcFgQmbNH"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG__AfY2TvvV"
      },
      "source": [
        "# Aquire some data\n",
        "\n",
        "As always we need to get our hands on some data. We will start by reusing some data sources from previous modules. In our first example we will use Wetlands from Uppsala.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD-AesWk4DtH"
      },
      "source": [
        "### Download wetlands\n",
        "This will give us three shapefiles and a pdf describing the data. We are mainly interested in VMI_C_Objekt_KLAR_2022 and VMI_C_Vatten_TOT_2022. The first contains the wetland polygons and the second contains polygons of water bodies within each wetland. VMI_C_Objekt_KLAR_2022 also contains our target variable which is the nature value classification of each wetland. We want to build a model that can predict the nature value of wetlands based on other data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WXVJPGVi5mJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "data_dir = '/workspace/data/'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "url = 'https://ext-dokument.lansstyrelsen.se/gemensamt/geodata/ShapeExport/Lsti.Inv_vmi_objekt.zip'\n",
        "filename = os.path.join(data_dir, 'Lsti.Inv_vmi_objekt.zip')\n",
        "\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o /workspace/data/Lsti.Inv_vmi_objekt.zip -d /workspace/data/wetlands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZYrjUUfz0aV"
      },
      "source": [
        "### Download ditches\n",
        "These are the same ditches as before. They were detected using deep learning on airborne laser data. If you have access to a computer where you can install [Qgis](https://qgis.org/) it is always a good idea to inspect the data visually as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki2DhgDwkEKl"
      },
      "outputs": [],
      "source": [
        "url = ('https:/geodata.naturvardsverket.se/nedladdning/Diken/Diken_Sverige/Diken_lansvis/Diken_I.zip')\n",
        "filename = '/workspace/data/Diken_I.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '/workspace/data/Diken_I.zip' -d //workspace/data//ditches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAo6peq_4Hvc"
      },
      "source": [
        "### Download high-valued forest areas\n",
        "These are key habitats that were judged to be extra valuable during field visits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6HXJ9rSKIJL"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = ('https://ext-dokument.lansstyrelsen.se/gemensamt/geodata/ShapeExport/Lsti.LstI_Inv_Skogliga_vardekarnor.zip')\n",
        "filename = '/workspace/data//Lsti.LstI_Inv_Skogliga_vardekarnor.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o //workspace/data//Lsti.LstI_Inv_Skogliga_vardekarnor.zip -d //workspace/data/valued_forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2nif8TkhPEz"
      },
      "source": [
        "### Download waterbodies as polygons\n",
        "These are polygons of both lakes and rivers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK3pkXWwhTf3"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = ('https://opendata-download.smhi.se/svar/Vattenytor_2016.zip')\n",
        "filename = '//workspace/data/Vattenytor_2016.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '//workspace/data/Vattenytor_2016.zip' -d /workspace/data/waterbodies\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aTRfoLV0WMe"
      },
      "source": [
        "### Read all data into geodataframes\n",
        "Due to the large amounts of data this can take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBKRuS2TxU9-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import geopandas as gpd\n",
        "# EPSG 3006 is the official swedish coordinate system that most swedish data will be develivred in.\n",
        "wetlands = gpd.read_file('/workspace/data/wetlands/Lsti.Inv_vmi_objekt.shp', crs='EPSG:3006') # These contain the targer variable.\n",
        "waterbodies = gpd.read_file('/workspace/data/waterbodies/Vattenytor_2016.shp')\n",
        "waterbodies = waterbodies.to_crs('EPSG:3006')\n",
        "ditches = gpd.read_file('/workspace/data/ditches/Diken_I.gpkg', crs='EPSG:3006') # Note that this is a geopackage\n",
        "ditches= ditches.to_crs('EPSG:3006')\n",
        "forest = gpd.read_file('/workspace/data/valued_forest/Lsti.LstI_Inv_Skogliga_vardekarnor.shp', crs='EPSG:3006')\n",
        "\n",
        "lakes = gpd.read_file('/workspace/data/waterbodies/Vattenytor_2016.shp', crs='EPSG:3006')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMd3DAunqJbE"
      },
      "source": [
        "### Task 1\n",
        "Inspect the data by using plots and commands you learn before in this course. The .info() and .plot() commands are useful here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu-TEOCrL6Oe"
      },
      "outputs": [],
      "source": [
        "forest.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC84LqeP-PGA"
      },
      "source": [
        "## Combine the data\n",
        "The next step is to combine the data into a single dataframe using a series of joins and spatial relates from module 6. We want a final dataframe containing the following:\n",
        "\n",
        "\n",
        "*   The lenght of ditches within each wetland\n",
        "*   Wetland area\n",
        "*   Area of wetland waterbodies\n",
        "*   Distance to nearest lake or river\n",
        "*   Distance to valuable forests\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jf8iRMmq7GW"
      },
      "outputs": [],
      "source": [
        "wetlands # Its the column Nv_klass_G that we want to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-PxMEhHq5YY"
      },
      "source": [
        "Start by droping columns we dont need to make it easier to follow the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6VX7hguq-7G"
      },
      "outputs": [],
      "source": [
        "columns_to_keep = ['OBJEKTNR', 'KLASS', 'geometry']\n",
        "wetlands = wetlands[columns_to_keep] # Drop columns not in columns_to_keep\n",
        "wetlands\n",
        "# class_names = ['1. very high nature valye', '2. high nature value', '3. some nature value', '4. low nature value']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flA2O613AhEi"
      },
      "source": [
        "Intersect the ditch lines with the wetland polygons and calculate the lenght of ditch channels within each wetland."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9lMww3U8dH-"
      },
      "outputs": [],
      "source": [
        "intersect = gpd.overlay(ditches, wetlands, how='intersection')\n",
        "intersect['ditch_length'] = intersect.geometry.length # Calculate length of intersecting ditch lines\n",
        "wetland_ditch_length = intersect.groupby('OBJEKTNR')['ditch_length'].sum().reset_index() # Aggregate lengths by wetland polygon\n",
        "wetlands_with_ditch_length = wetlands.merge(wetland_ditch_length, on='OBJEKTNR', how='left').fillna({'ditch_length': 0})\n",
        "wetlands_with_ditch_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmAUrZrkI3_c"
      },
      "source": [
        "Intersect the wetland waterbodies polygons with the wetland polygons and calculate the waterbody area within each wetland."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbzVQ1CVyAFT"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "intersect = gpd.overlay(waterbodies, wetlands, how='intersection')\n",
        "intersect['water_area'] = intersect.geometry.area # Calculate water area for intersecting waterbodies\n",
        "area_waterbodies = intersect.groupby('OBJEKTNR')['water_area'].sum().reset_index()\n",
        "wetlands_with_waterbodies = wetlands.merge(area_waterbodies, on='OBJEKTNR', how='left').fillna({'water_area': 0})\n",
        "wetlands_with_waterbodies # notice the new column \"water_area\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbXlRgB13ENL"
      },
      "source": [
        "For lakes and valuable forests we need to calculate the distance to nearest lake instead of intersecting wetlands and lakes.\n",
        "\n",
        "Start with lakes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adNILAIWxHtr"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "from scipy.spatial import cKDTree\n",
        "\n",
        "wetlands_with_lake_distance = wetlands.copy()\n",
        "lake_spatial_index = cKDTree(lakes.centroid.apply(lambda x: (x.x, x.y)).tolist()) # Create a spatial index for the lake centroids\n",
        "\n",
        "# Function to calculate the nearest distance\n",
        "def nearest_distance(point):\n",
        "    distance, idx = lake_spatial_index.query((point.x, point.y))\n",
        "    return distance\n",
        "\n",
        "wetlands_with_lake_distance['nearest_lake_distance'] = wetlands_with_lake_distance.centroid.apply(nearest_distance) # Apply the function to each wetland centroid to get the nearest distance to a lake\n",
        "wetlands_with_lake_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrYX6k89lrtr"
      },
      "source": [
        "Then do the same with valuable forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoMcpDxY9ZjO"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "from scipy.spatial import cKDTree\n",
        "\n",
        "wetlands_with_forest_distance = wetlands.copy()\n",
        "forest_spatial_index = cKDTree(forest.centroid.apply(lambda x: (x.x, x.y)).tolist()) # Create a spatial index for the lake centroids\n",
        "\n",
        "# Function to calculate the nearest distance\n",
        "def nearest_forest_distance(point):\n",
        "    distance, idx = forest_spatial_index.query((point.x, point.y))\n",
        "    return distance\n",
        "\n",
        "wetlands_with_forest_distance['nearest_forest_distance'] = wetlands_with_forest_distance.centroid.apply(nearest_forest_distance) # Apply the function to each wetland centroid to get the nearest distance to a lake\n",
        "wetlands_with_forest_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmwu3Mx_nlwJ"
      },
      "source": [
        "Finally you need to join all geodataframes. The data can be joined based on the attribute 'OBJEKTNR'. This should give you a dataframe with ditch lenght, area of lakes within each wetland, distance to nearest lake, distance to nearest valuable forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOQcvSOl3I89"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "attribute_dataframes = [wetlands_with_ditch_length, wetlands_with_waterbodies, wetlands_with_lake_distance, wetlands_with_forest_distance]\n",
        "merged_wetlands = wetlands.copy()\n",
        "\n",
        "# Merge the dataframes one by one based on the ID of each wetland\n",
        "for df in attribute_dataframes:\n",
        "    cols_to_use = df.columns.difference(merged_wetlands.columns)\n",
        "    merged_wetlands = pd.merge(merged_wetlands, df[cols_to_use], left_index=True, right_index=True, how='outer')\n",
        "merged_wetlands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHMRle9vATLR"
      },
      "source": [
        "The final step is to drop the column for the ID and geometry so the machine learning model does not attempt to train on those attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puHS6b5WAa3b"
      },
      "outputs": [],
      "source": [
        "clean_data = merged_wetlands.drop(['geometry', 'OBJEKTNR'], axis=1)\n",
        "clean_data = clean_data.reset_index(drop=True)\n",
        "clean_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zkrlxRQoW2q"
      },
      "source": [
        "# Machine learning\n",
        "Now when we have a dataframe with wetlands and some attributes we can use them to train a model that can predict the nature value on mires. We will use [scikit-learn](https://scikit-learn.org/stable/) to build and test a basic random forest model. To evaluate weather the model is good or not we will split the data into training 80% and testing 20%.\n",
        "\n",
        "I highly encourage you to take some time to learn more about scikit-learn if you are interested in working with machine learning in the future. Here is a longer video if you want to learn more: [Long form video by Vincent](https://www.youtube.com/watch?v=0B5eIE_1vpU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKINCm_r5XLT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = clean_data.iloc[:,0] # This is Nature value KLASS\n",
        "x = clean_data.iloc[:,1:] # These are all the other attributes variables\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0, stratify = y) # splits the data into training and testing data. test_size=0.2 means that 20% of the wetlands will be set aside for testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3pLL7SVGuJf"
      },
      "source": [
        "## Decision tree\n",
        "It is always good to start with a simple model so we will build a [decision tree](https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using or training data and test it on our test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3NsXiFLG6eR"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(max_depth=3) # This number determains how many decisions the tree will contain\n",
        "clf.fit(x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2eLc-vDH2CN"
      },
      "source": [
        "We can also inspect the trained decision tree to see whats going on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNDjqKc9HyAE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plot_tree(\n",
        "    clf,\n",
        "    feature_names=x.columns,\n",
        "    class_names=[str(c) for c in clf.classes_],  # convert to strings\n",
        "    filled=True,\n",
        "    fontsize=8\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qGB8ElBHRgr"
      },
      "source": [
        "The first decision is on the lenght of ditch channels on a wetland.\n",
        "\n",
        "The standard way to evaluate a machine learning model is to use it to predict the test data and then compare the prediction to the test labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY8QIRTRHUiQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = clf.predict(x_test) # clf is the name of our model defined above and .predict means that we use the model in prediction mode.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3-71L8hHk3H"
      },
      "source": [
        "The accuracy is between 0 and 1 where 1 is 100% and something like 0.1 would be 10% accurate. In other words, the model would predict the right nature value class 10% of the time. For more detailed information we can use a classification report to see how the model preforms on different classes. The classification report produces values for precision, recall, f1-score and support. Lets break those down a bit. using class 1 (Mkt högt natuvärde/very high nature value).\n",
        "\n",
        "\n",
        "*   Precision = % of how many of the predicted wetlands in class 1 that were actually in class 1.\n",
        "*   Recall = % of all wetlands in class 1 that the model predicted as class 1.\n",
        "*   f1-score = A combination of precision and recall. If your model has high precision but low recall, or vice versa, the F1-score will be lower. Higher number is better.\n",
        "*   support = How many samples were in that class in the test data.\n",
        "\n",
        "\n",
        "This might be alot to take in so focus on the f1-score for now. we want as high f1-score as possible. You can also run the prediction on the same data that it was trained on to compare that with the prediction on the test data. If the difference is big then your model has overfitted to the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orNh2Qwg7DKr"
      },
      "outputs": [],
      "source": [
        "# test the model on its own training data\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = clf.predict(x_train)\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFwCx6Qn0HBe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_test = clf.predict(x_test)\n",
        "print(classification_report(y_test, y_pred_test, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jaakk4F16Ct"
      },
      "source": [
        "Our model seems to be best at classifying wetlands with low values \"Lågt naturvärde\".\n",
        "\n",
        "Another way to dive deeper into the model output is to look at a confusion matrix where each prediction is compared to the actual class of that wetland. Each square will show the number of correct predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz6va6bU0MqD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "# Get the class names\n",
        "class_names = ['1. very high nature valye', '2. high nature value', '3. some nature value', '4. low nature value']   # Replace with your class names\n",
        "\n",
        "# Visualize the confusion matrix with class names\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj6wQXM4M5Av"
      },
      "source": [
        "### Task 2\n",
        "Change the tree depth to see if that improves the accuracy of the model. Inspect the f1-score and confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av5xc538C2Ue"
      },
      "source": [
        "## Decision forest\n",
        "Now when you have seen a decision tree it's time to build a decision forest. It is the same thing but with more trees working together. The parameter in the code below \"n_estimators=2\" determains the number of trees in the forest. In this case it will train an ensabmle of 2 trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tNOrhZWC-yJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=200, max_depth=3) # create a random forest model\n",
        "rf_clf.fit(x_train, y_train) # train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pqRV1p3DLgT"
      },
      "outputs": [],
      "source": [
        "y_pred = rf_clf.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ff6YgysNe0w"
      },
      "source": [
        "We can also inspect the decision trees in a random forest but if the forest contains alot of trees this can be a bit tedious."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJAIlWxbNeSh"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "from IPython.display import Image\n",
        "\n",
        "tree_index = 0  # Change this to select a different tree. Inspect the second tree in the forest by changing 0 to 1.\n",
        "tree = rf_clf.estimators_[tree_index]\n",
        "\n",
        "dot_data = export_graphviz(tree, out_file=None,\n",
        "                           feature_names=x.columns,\n",
        "                           class_names=[str(c) for c in clf.classes_],  # convert to strings\n",
        "                           filled=True, rounded=True)\n",
        "\n",
        "# Create a Graphviz object\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "Image(graph.create_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5eCusrD453o"
      },
      "source": [
        "With random forest we can also get an idea of what attributes are important for the model using feature importance. Keep in mind that machine learning is a bit of a black box so in order to see why a feature is important you would have to inspect all the decision trees in a random forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keErHczg5HTu"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "feature_importance = rf_clf.feature_importances_\n",
        "feature_names = x.columns\n",
        "indices = feature_importance.argsort()[::-1]\n",
        "\n",
        "# create a dataframe for nicer plot\n",
        "importance_df = pd.DataFrame({'Feature': feature_names[indices],\n",
        "                              'Importance Score': feature_importance[indices]})\n",
        "\n",
        "# higher values are more important for the model\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance Score', y='Feature', data=importance_df, hue='Feature', palette='viridis', legend=False)\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MELVt3iNMyP9"
      },
      "source": [
        "### Task 3\n",
        "Train a random forest model with more trees and see if that improves the result. Can you think of a reason to not build as many trees as possible?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCIR7yp86QM4"
      },
      "source": [
        "# Implement your model\n",
        "You have now trained a model and evaluated the preformance of the model. The next step is to combine the prediction with the original dataframe. In this case the geodataframe with all the combined data \"merged_wetlands\". Note that we want to keep the geometry and ID so we can save the result as a geopackage or shapefile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIQYUc-08cLT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "subset_df = merged_wetlands[x_train.columns] # Subset the DataFrame to include only the columns used for training\n",
        "\n",
        "predictions = rf_clf.predict(subset_df) # run the prediction\n",
        "predictions_df = pd.DataFrame(predictions, columns=['Prediction']) # Convert predictions to a DataFrame\n",
        "\n",
        "final_df = pd.concat([merged_wetlands, predictions_df], axis=1) # merge predictions with the original geodataframe\n",
        "final_df.to_file('/content/predicted_wetlands.gpkg', driver='GPKG')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFI8NCzsFonu"
      },
      "source": [
        "### Visualize the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5kuPNBoB4_x"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'final_df' is your GeoDataFrame and 'Prediction' and 'KLASS' are the columns you want to use for coloring\n",
        "\n",
        "# Plot the first GeoDataFrame (colored by 'Prediction') on the first axis\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# Plot the second GeoDataFrame (colored by 'KLASS') on the second axis\n",
        "final_df.plot(column='KLASS', cmap='viridis', ax=ax1, legend=True)\n",
        "ax2.set_title('Original nature value')\n",
        "\n",
        "final_df.plot(column='Prediction', cmap='viridis', ax=ax2, legend=True)\n",
        "ax1.set_title('Predicted nature value')\n",
        "\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FAogvcRFrpF"
      },
      "source": [
        "### Task 4\n",
        "Start from the dataframe merged_wetlands and calculate wetland area and perimeter. Then devide area by perimeter to get wetland shape complexity and finally train a new random forest model with the three new variables included. If you want to explore a bit more i recomend going back to module 5 and extract NDVI to each wetland polygon (not mandatory but fun)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt2DGIsUDoki"
      },
      "source": [
        "# Neural networks\n",
        "\n",
        "Neural networks are designed to mimic the behavior of the human brain. It consists of interconnected nodes called neurons, organized into layers. Each neuron receives input, processes it, and passes the output to other neurons.\n",
        "\n",
        "*    Input layer: This layer receives the initial data or features that are fed into the neural network. Each neuron in the input layer represents a feature of the input data.\n",
        "\n",
        "*    Hidden layers: These layers sit between the input and output layers. They perform computations on the input data using weighted connections between neurons. Hidden layers enable the neural network to learn complex patterns and relationships within the data.\n",
        "\n",
        "*    Output layer: This layer produces the final output of the neural network. The number of neurons in the output layer depends on the task the network is designed for. For example, in a classification task, each neuron in the output layer may represent a different class, in our case its wetland classification.\n",
        "\n",
        "We can build simple neural networks using sklearn. Lets start by building a neural network with one hidden layer with five neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id_cX26u8XXz"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# define and train the model hidden_layer_sizes=(5) means that the hidden layer will have 5 neurons.\n",
        "nnet = MLPClassifier(solver='lbfgs', max_iter=1000, hidden_layer_sizes=(5), random_state=1)\n",
        "nnet.fit(x_train, y_train)\n",
        "\n",
        "# evaluate the trained model on test data\n",
        "y_pred = nnet.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN4EqsHfF2Lt"
      },
      "source": [
        "To better understand what this is we can install a small package for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78g0fIRrDKuk"
      },
      "outputs": [],
      "source": [
        "!pip install nnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPrnE7YrG7GS"
      },
      "source": [
        "Lets plot the neural network we just trained. It has four input nodes and four output nodes, one for each wetland class. It only have one hidden layer with five nodes. The difference between machine learning and deep learning is how many hidden layers the model has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTQf6XFrDNlN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
        "from nnv import NNV\n",
        "\n",
        "layersList = [\n",
        "    {\"title\":\"input\\n\", \"units\": 4},\n",
        "    {\"title\":\"hidden 1\\n\", \"units\": 5}, # This is the hidden layer\n",
        "    {\"title\":\"output\\n\", \"units\": 4,},\n",
        "]\n",
        "\n",
        "NNV(layersList, max_num_nodes_visible=10,font_size=11).render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aUY1PpIHXyp"
      },
      "source": [
        "Lets build a deep neural network using two hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKCgpk0JHcuw"
      },
      "outputs": [],
      "source": [
        "nnet = MLPClassifier(solver='lbfgs', max_iter=1000, hidden_layer_sizes=(5,5), random_state=1)\n",
        "nnet.fit(x_train, y_train)\n",
        "\n",
        "# evaluate the trained model on test data\n",
        "y_pred = nnet.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOvYBI6KH4IA"
      },
      "source": [
        "This is how the new neural network looks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agq4qHC7H6sf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
        "from nnv import NNV\n",
        "\n",
        "layersList = [\n",
        "    {\"title\":\"input\\n\", \"units\": 4},\n",
        "    {\"title\":\"hidden 1\\n\", \"units\": 5},\n",
        "    {\"title\":\"hidden 2\\n\", \"units\": 5},\n",
        "    {\"title\":\"output\\n\", \"units\": 4,},\n",
        "]\n",
        "\n",
        "NNV(layersList, max_num_nodes_visible=10,font_size=11).render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzqs9ISkNdl4"
      },
      "source": [
        "### Task 5\n",
        "Our dataset is not optimal for neural networks since its quite limited and we have not rescaled any of our attributes. Play around with the number of neurons and hidden layers and see if you can get better accuracy than with the basic decision tree model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfHPk0EGSgXD"
      },
      "source": [
        "# Please help me with my research\n",
        "### Task 6\n",
        "I have created a dataset with AI-predicted road culverts but most of them are not real. I need to filter out the false positives and you can help me explore the impact of flow accumulation using machine learning. The shapefile contains the columns FID1, VALUE, FLowacc_min, Flowacc_max, Flowacc_median, Flowacc_standard_deviation, Real and geometry. I do not remember what VALUE means so drop FID and VALUE from the dataset. The column Real has 0 and 1s. 0 means that the culvert did not exist in real life and 1 means that it did. We can call these False positive and True positive. This column is your target. Use machine learning and try to remove the false positive culverts without dropping to many of the true positive culverts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQm6PzL9CZ9F"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1R-OCjlipqpk8_EJZHiyF6iWag0Cur82-\"\n",
        "output = \"/content/mergeZonalExport.zip\"   # rename as needed\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip -o '/content/mergeZonalExport.zip' -d /content/culverts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVULSEaCHIh9"
      },
      "source": [
        "Keep in mind that Real is the 5th or 6th column so you need to set the right column as target for your modeling. It's completly fine to use an LLM here. Good luck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqzW0SAKDG8_"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpKrHR2iMBh9"
      },
      "source": [
        "# Conclusion\n",
        "Machine learning is a great tool to quickly build predictive models. With vector data we can calculate spatial relations such as distance and area using geopandas. Since machine learning readily can be applied to attribute tables we can combine geopandas with machine learning for predictions.\n",
        "\n",
        "One strenght of decision trees and random forest is that they do not expect or require the data to be normaly distributed. You can even mix classified data such as land use with continues values such as distances.\n",
        "\n",
        "We will work more with deep learning in module nine but generally a random forest is much easier to work with and often produce similar results on tabular data.\n",
        "\n",
        "Look into the sklearn documentation for more inspiration on other machine learning methods: https://scikit-learn.org/stable/user_guide.html\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
