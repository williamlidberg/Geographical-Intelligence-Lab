{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtcb3r/OTcoJ5n7J2azSoE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamlidberg/Geographical-Intelligence-Lab/blob/main/notebooks/machine_learning_raster_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning on raster data\n",
        "Start by installing geopandas and cloning the course github repositroy where some example data is stored. The raster data will be stored under /content/Analyses-of-Environmental-Data-2/data/rasters and the field plots will be stored as csv files under under /content/Analyses-of-Environmental-Data-2/data/\n"
      ],
      "metadata": {
        "id": "YmITriRfBBn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas\n",
        "!git clone https://github.com/williamlidberg/Analyses-of-Environmental-Data-2.git # This include some test data we will use"
      ],
      "metadata": {
        "id": "NN5pB4_2BUv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import and inspect the field data csv"
      ],
      "metadata": {
        "id": "0sF38y1av2Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "soildata = pd.read_csv('/content/Analyses-of-Environmental-Data-2/data/Krycklan_Soilsurvey_data.csv', sep=';')\n",
        "soildata"
      ],
      "metadata": {
        "id": "EVTPFVMDxB25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the data"
      ],
      "metadata": {
        "id": "RO_NdbHRxODT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5etYJErt9bMe"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "soildata_gdf = gpd.GeoDataFrame(soildata, geometry=gpd.points_from_xy(soildata.East, soildata.North), crs=3006)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,20)\n",
        "soildata_gdf.plot(column='SMC', cmap='viridis_r', legend=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rasterdata\n",
        "\n",
        "In order to train a machine learning model you need some geospatial data.\n",
        "The following raster layers were calculated from the digital elevation model using whitebox tools.\n",
        "\n",
        "1.   DownslopeIndex_2m\n",
        "2.   DownslopeIndex_4m\n",
        "3.   DepthToWater_1ha\n",
        "4.   DepthToWater_2ha\n",
        "5.   DepthToWater_4ha\n",
        "6.   DepthToWater_8ha\n",
        "7.   DepthToWater_16ha\n",
        "8.   DepthToWater_32ha\n",
        "9.   ElevationAboveStream_1ha\n",
        "10.  ElevationAboveStream_2ha\n",
        "11.  ElevationAboveStream_4ha\n",
        "12.  ElevationAboveStream_8ha\n",
        "13.  ElevationAboveStream_16ha\n",
        "14.  ElevationAboveStream_32ha\n",
        "15.  PennocLandformClassification\n",
        "16.  PlanCurvature\n",
        "17.  RelativeTopographicPosition\n",
        "18.  TopographicWetnessIndex\n",
        "19.  WILT\n",
        "20.  DEM\n",
        "21.  Slope\n",
        "22.  DInfFlowaccumulation\n",
        "\n",
        "You need to extract the pixel values to the field plots. This can be done using a combination of [rasterio](https://rasterio.readthedocs.io/en/latest/) and [geopandas](https://geopandas.org/en/stable/). rasterio is a python package that focuses on reading and writing raster data. Start by installing it in your environment."
      ],
      "metadata": {
        "id": "fojAezvhxmez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "OfeayqXgv5VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we export anything its a good habit to inspect some of the data to make sure it looks like expected."
      ],
      "metadata": {
        "id": "mtMkVKfq1TGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "dem = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif') # dem is short for digital elevation model\n",
        "show(dem, cmap='viridis_r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yiFZq0331dcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1\n",
        "Plot some of the raster files under /content/Analyses-of-Environmental-Data-2/data/rasters/ so you get a sense of what the data is representing.\n",
        "\n",
        "Note that some muppet (me) has mixed lower case and upper case letters in the names and python is case sensetive. slope and Slope are not the same. This is a very common mistake."
      ],
      "metadata": {
        "id": "ZmB2pIr91ut3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract raster values to field plots\n",
        "If the data looks to be in order we can use raster io to extract the raster values to our field plots. This code first finds the x and y coordinates of each field plot in the geodataframe. \"coords = [(x,y) for x, y in zip(soildata_gdf.geometry.x, soildata_gdf.geometry.y)]\" it then loops over each field plot and extracts the raster values. Finally it adds the extracted values to a new column in the geodataframe. \"soildata_gdf['dem'] = [x[0] for x in src.sample(coords)]\""
      ],
      "metadata": {
        "id": "wbP_oHkV1rbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "import geopandas as gpd\n",
        "\n",
        "\n",
        "coords = [(x,y) for x, y in zip(soildata_gdf.geometry.x, soildata_gdf.geometry.y)]\n",
        "\n",
        "# Open the raster using rasterio and extract the pixel values to the geodataframe\n",
        "# dem\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif')\n",
        "soildata_gdf['dem'] = [x[0] for x in src.sample(coords)] # Naming is important to keep things in order\n",
        "# Slope\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/Slope/16m.tif')\n",
        "soildata_gdf['Slope'] = [x[0] for x in src.sample(coords)]\n",
        "# RelativeTopographicPosition\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/RelativeTopographicPosition/16m.tif')\n",
        "soildata_gdf['RelativeTopographicPosition'] = [x[0] for x in src.sample(coords)]\n",
        "# TopographicWetnessIndex\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/TopographicWetnessIndex/16m.tif')\n",
        "soildata_gdf['TopographicWetnessIndex'] = [x[0] for x in src.sample(coords)]\n",
        "# DownslopeIndex_2m\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/DownslopeIndex_2m/16m.tif')\n",
        "soildata_gdf['DownslopeIndex_2m'] = [x[0] for x in src.sample(coords)]\n",
        "# DepthToWater_1ha\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/DepthToWater_1ha/16m.tif')\n",
        "soildata_gdf['DepthToWater_1ha'] = [x[0] for x in src.sample(coords)]\n"
      ],
      "metadata": {
        "id": "CMozRvMXwqsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now you have a new geodataframe with both the field data and the raster data"
      ],
      "metadata": {
        "id": "gpLjmCQzz0l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soildata_gdf"
      ],
      "metadata": {
        "id": "M5bnDEvUyrPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be useful to plot one of the variables to see if it makes any sense. compare this plot to the raster plots you did above."
      ],
      "metadata": {
        "id": "fbM2CvmV0q4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,20)\n",
        "soildata_gdf.plot(column='dem', cmap='viridis_r')"
      ],
      "metadata": {
        "id": "VZi6udFHz8O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are only interested in soil moisture now we will drop the other Y-variables. We also need to split the data into training data and testing data. The model will be trained on the training data and evaluated on the test data just like in module 7."
      ],
      "metadata": {
        "id": "QlhxV18D4Weo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soildata_clean = soildata_gdf.drop(soildata_gdf.columns[2:10], axis=1) # drops column 3 to 10\n",
        "soildata_clean = soildata_clean.drop(soildata_gdf.columns[0], axis=1) # drops column 0 which is the text for the soil moisture\n",
        "soildata_clean # SMC_code \t= Soil moisture code"
      ],
      "metadata": {
        "id": "koWTdsAW4vP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into training and testing\n",
        "Here we will use stratified sampling which means that sklearn will include examples of all classes in both the training data and the testing data."
      ],
      "metadata": {
        "id": "7xszut6E7m_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = soildata_clean.iloc[:,0] # This is soil moisture\n",
        "x = soildata_clean.iloc[:,1:] # These are all the topographical variables\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0, stratify = y)"
      ],
      "metadata": {
        "id": "6b29sSrd4Tn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning the hyperparameters\n",
        "In the field of machine learning things are named differntly than in traditional statistics. In statistics the settings for a model is sometimes refered to as features. However, in machine learning the features are the data you extracted to the points and the setting of the model is instead called hyperparameters. Much cooler. It is quite common to fiddle with these hyper paramters to see what works and this process can be autmated. This is known as tuning the hyperparameters.\n",
        "\n",
        "Here is an example using a tune grid where multiple models will be trained using all possible combinations of the settings listed bellow. This is a brute force approach and very demanding of your hardware. But computer time is cheaper than human time so lets do it."
      ],
      "metadata": {
        "id": "h8es3eq4yyy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "model = RandomForestClassifier() # note that we are using classification for the soil moisture classes\n",
        "\n",
        "\n",
        "tune_grid = {'n_estimators': [50, 100, 500],\n",
        "               'max_features': ['sqrt'],\n",
        "               'max_depth': [4,5,6],\n",
        "               'min_samples_split': [2, 5, 10],\n",
        "               'min_samples_leaf': [1, 2, 4],\n",
        "               'bootstrap': [True]}\n",
        "\n",
        "rf_random = RandomizedSearchCV(estimator = model, param_distributions = tune_grid, random_state=0, n_jobs = -1)\n",
        "\n",
        "# Train the model using the optimal hyperparameters\n",
        "rf_random.fit(x_train, y_train)\n",
        "print('The best combination of hyperparameters was', rf_random.best_params_)"
      ],
      "metadata": {
        "id": "03uB9Zdu72Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model just like in module 7. Note that the accuracy is between 0 and 1 so 0.5 is 50%."
      ],
      "metadata": {
        "id": "g3HTnLAo0B3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = rf_random.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "QsLXsJCd8T2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the F1-score for each soil moisture class and pay attention to the support column which shows how many plots are within each category. It's harder to learn with few examples from a class. Remember that the soil moisture classes were\n",
        "\n",
        "\n",
        "*   1 = Dry\n",
        "*   2 = Mesic\n",
        "*   3 = Mesic-moist\n",
        "*   4 = Moist\n",
        "*   2 = Wet"
      ],
      "metadata": {
        "id": "0JQtfUDr0Qad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_test = rf_random.predict(x_test)\n",
        "print(classification_report(y_test, y_pred_test, zero_division=0))"
      ],
      "metadata": {
        "id": "VoI1fiFX8IbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement a machine learning model on raster data\n",
        "Now we have a working model that we want to apply to the Krycklan catchment. We need to read all the rasterlayers, apply the model and then save the result as a new raster.\n",
        "\n",
        "All rasterdata will be read into numpy arrays using gdal."
      ],
      "metadata": {
        "id": "aWrmvVB28kcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from osgeo import gdal_array\n",
        "import numpy as np\n",
        "# Read raster data as numeric array from file\n",
        "dem = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif')\n",
        "Slope = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/Slope/16m.tif')\n",
        "RelativeTopographicPosition = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/RelativeTopographicPosition/16m.tif')\n",
        "TopographicWetnessIndex = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/TopographicWetnessIndex/16m.tif')\n",
        "DownslopeIndex_2m = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/DownslopeIndex_2m/16m.tif')\n",
        "DepthToWater_1ha = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/DepthToWater_1ha/16m.tif')\n"
      ],
      "metadata": {
        "id": "o7Fsllox8nQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a list of all arrays you wish to include. Note that you need to add or remove the variables in both the list and the converted dataframe."
      ],
      "metadata": {
        "id": "yp1tb_PW9kkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of all arrays. you can\n",
        "list_or_all_rasters = [dem, Slope, RelativeTopographicPosition, TopographicWetnessIndex, DownslopeIndex_2m, DepthToWater_1ha]\n",
        "\n",
        "all_data = np.array(list_or_all_rasters)\n",
        "all_data=all_data.reshape(6,738*662).T # 6 is the number of indices in the list and 738*662 is the shape of the original DEM\n",
        "\n",
        "df_data=pd.DataFrame(all_data,columns=['dem', 'Slope', 'RelativeTopographicPosition','TopographicWetnessIndex', 'DownslopeIndex_2m', 'DepthToWater_1ha'])\n"
      ],
      "metadata": {
        "id": "QYR4m7pJ9i6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the classification"
      ],
      "metadata": {
        "id": "AJTYGeTNZxfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = rf_random.predict(df_data)\n",
        "\n",
        "# Save the data as a raster file with coordinates and extent from one of the input layers\n",
        "result = result.reshape(738,662)\n",
        "extent = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif')\n",
        "\n",
        "with rasterio.Env():\n",
        "  profile = extent.profile\n",
        "  with rasterio.open('/content/prediction.tif', 'w', **profile) as dst:\n",
        "        dst.write(result, 1)\n",
        "\n",
        "src = rasterio.open('/content/prediction.tif')\n",
        "plt.imshow(src.read(1), cmap='viridis_r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cl1vzwYc-Hpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the probability of a class\n",
        "Nature is realy fixed into classes so the probability of a class might actually be more useful. Instead of saving the classification from the model we can save the probability of a class."
      ],
      "metadata": {
        "id": "XRnLpyeP-88I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = rf_random.predict_proba(df_data) # notice that we use predict_proba instead of predict\n",
        "result = result[:, 4]\n",
        "# Save the data as a raster file with coordinates and extent from one of the input layers\n",
        "result = result.reshape(738,662)\n",
        "extent = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif')\n",
        "\n",
        "with rasterio.Env():\n",
        "  profile = extent.profile\n",
        "  with rasterio.open('/content/prediction_probability.tif', 'w', **profile) as dst:\n",
        "        dst.write(result, 1)\n",
        "\n",
        "src = rasterio.open('/content/prediction_probability.tif')\n",
        "plt.imshow(src.read(1), cmap='viridis_r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_T4mZ_yV-LFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "Plot the probability of Dry areas, Wet areas and Mesic-moist areas. The original values for the classes are:\n",
        "\n",
        "*   1 = Dry\n",
        "*   2 = Mesic\n",
        "*   3 = Mesic-moist\n",
        "*   4 = Moist\n",
        "*   2 = Wet\n",
        "\n",
        "But remember that python lists starts from 0 to class 1 is at position 0 in the probability output."
      ],
      "metadata": {
        "id": "bqE1rU1TalEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2\n",
        "Train a decision tree model like the one you used in module 7 on the same data and compare it to the random forest model."
      ],
      "metadata": {
        "id": "8p1FzW4i03aj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BDlyUTA1Eub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3\n",
        "Predict carbon to nitrogren ratio based on topographical data\n",
        "---\n",
        "\n",
        "Now you will do the same as above but instead of using classified soil moisture you will try to predict the C/N ratio from a new set of field plots from Krycklan. The data can be found here: /content/Analyses-of-Environmental-Data-2/data/Krycklan_Chemdata.csv The share of carbon between mineral-associated and particulate organic matter and the ratio between carbon and nitrogen affect soil carbon stocks and mediate the effects of other variables on soil carbon stocks.\n",
        "\n",
        "\n",
        "\n",
        "This dataset contains samples from multiple depths from each plot. You can select sample depth by chemdata_gdf= chemdata_gdf[chemdata_gdf['SampleDepth'] == 0] to get the surface sample.\n",
        "\n",
        "\\\n",
        "Remember to change from  [classification](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforest#sklearn.ensemble.RandomForestClassifier) to [regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=randomforest#sklearn.ensemble.RandomForestRegressor) and do not use stratified sampling when splitting the data.\n",
        "\n",
        "Change\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0, stratify = y)\n",
        "\n",
        "to\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0)\n",
        "    \n",
        "Drop the apropriate columns before training the model. You will also need to use other metrics when evaluating.\n",
        "\n",
        "These are some examples that you can use:\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    # Calculate mean absolute error (MAE)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "    # Calculate mean squared error (MSE)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "    # Calculate root mean squared error (RMSE)\n",
        "    rmse = mse ** 0.5\n",
        "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "    # Calculate R-squared (R2)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(\"R-squared (RÂ²):\", r2)"
      ],
      "metadata": {
        "id": "5ZpK4C6M_Cfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Kj6sMGNSUe7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4\n",
        " Extract more variables with Whitebox Tools\n",
        "---\n",
        "[Whitebox Tools](https://www.whiteboxgeo.com/manual/wbt_book/available_tools/geomorphometric_analysis.html) Is a great software topographical modeling. This section describes how to extract additional topographical features. This is an example on how to set up Whitebox Tools and extract aspect from the original DEM. Your task is to extract a new topographical index using whitebox tools and include it with the other data and train a machine learning model where the new index is included.\n",
        "\n",
        "To do this you need to complete the following steps\n",
        "\n",
        "\n",
        "1.   Calculated the index using whitebox.\n",
        "2.   Extract raster values to the field plot points.\n",
        "3.   Add it to the list of rasters for inference and dont forget that it needs to have the same name in the table as in the list of raters.\n",
        "\n"
      ],
      "metadata": {
        "id": "5M0d3H9M_d83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whitebox\n",
        "\n",
        "import whitebox\n",
        "\n",
        "wbt = whitebox.WhiteboxTools()\n",
        "\n",
        "wbt.ruggedness_index(\n",
        "    dem = '/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif',\n",
        "    output = '/content/Analyses-of-Environmental-Data-2/data/ruggedness.tif'\n",
        ")\n"
      ],
      "metadata": {
        "id": "P2x9ja2cbzrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the tasks and upload the notebook with your name to canvas."
      ],
      "metadata": {
        "id": "X7XMM9Rs_Vsc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmaleQy2OXGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}