{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj/MHtcilNQ2w8Oey7JshS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamlidberg/Geographical-Intelligence-Lab/blob/main/notebooks/Machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning on vector data\n",
        "\n",
        "The core idea of machine learning is to program a system using data instead of with code. There are multiple types of machine learning which are used for different types of data such as images, text or numbers, but there are two main subdomains: traditional machine learning and deep learning. Machine learning was introduced around 1940 and deep learning 1963 but it was mainly deep learning that exploded in the 2000s. Deep learning is great but it is not always the best solution, especially when working with tablular data (tables with rows and columns). Therefore, this module will introduce you to traditional machine learning on geopandas dataframes.\n",
        "Random forest\n",
        "\n",
        "Random forests is a very robust machine learning method which I both love and hate. I love it because it is so easy to use and always provide a good baseline. I hate it because it is hard to come up with something more powerful and novel.\n",
        "\n",
        "Random forest can be used for both classification and regression and works by building many decicion trees on randomly selected parts of your dataframe. Multiple deicion trees makes a decision forest hence the name random forest."
      ],
      "metadata": {
        "id": "Uf9WPZ7YR9f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download wetlands\n",
        "This will give us three shapefiles and a pdf describing the data. We are mainly interested in VMI_C_Objekt_KLAR_2022 and VMI_C_Vatten_TOT_2022. The first contains the wetland polygons and the second contains polygons of water bodies within each wetland. VMI_C_Objekt_KLAR_2022 also contains our target variable which is the nature value classification of each wetland. We want to build a model that can predict the nature value of wetlands based on other data."
      ],
      "metadata": {
        "id": "UOMKCw78SHyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "# Wetlands\n",
        "url = ('https://ext-dokument.lansstyrelsen.se/gemensamt/geodata/ShapeExport/Lsti.Inv_vmi_objekt.zip')\n",
        "filename = 'Lsti.Inv_vmi_objekt.zip'\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o /content/Lsti.Inv_vmi_objekt.zip -d /content/wetlands\n",
        "# Ditches\n",
        "url = ('https://geodata.naturvardsverket.se/nedladdning/Diken/Diken_Sverige/Diken_lansvis/Diken_I.zip')\n",
        "filename = '/content/Diken_I.zip'\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '/content/Diken_I.zip' -d /content/ditches\n",
        "# Valuable forest\n",
        "url = ('https://ext-dokument.lansstyrelsen.se/gemensamt/geodata/ShapeExport/Lsti.LstI_Inv_Skogliga_vardekarnor.zip')\n",
        "filename = 'Lsti.LstI_Inv_Skogliga_vardekarnor.zip'\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o /content/Lsti.LstI_Inv_Skogliga_vardekarnor.zip -d /content/valued_forest\n",
        "# Water surfaces\n",
        "url = ('https://opendata-download.smhi.se/svar/Vattenytor_2016.zip')\n",
        "filename = '/content/Vattenytor_2016.zip'\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '/content/Vattenytor_2016.zip' -d /content/waterbodies"
      ],
      "metadata": {
        "id": "sWBfuUpfRnIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "# EPSG 3006 is the official swedish coordinate system that most swedish data will be develivred in.\n",
        "wetlands = gpd.read_file('/content/wetlands/Lsti.Inv_vmi_objekt.shp', crs='EPSG:3006') # These contain the targer variable.\n",
        "waterbodies = gpd.read_file('/content/waterbodies/Vattenytor_2016.shp')\n",
        "waterbodies = waterbodies.to_crs('EPSG:3006')\n",
        "ditches = gpd.read_file('/content/ditches/Diken_I.gpkg', crs='EPSG:3006') # Note that this is a geopackage\n",
        "ditches= ditches.to_crs('EPSG:3006')\n",
        "forest = gpd.read_file('/content/valued_forest/Lsti.LstI_Inv_Skogliga_vardekarnor.shp', crs='EPSG:3006')\n",
        "lakes = gpd.read_file('/content/waterbodies/Vattenytor_2016.shp', crs='EPSG:3006')"
      ],
      "metadata": {
        "id": "fO4gYvgASfsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wetlands # Its the column Nv_klass_G that we want to predict."
      ],
      "metadata": {
        "id": "2Jf8iRMmq7GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by droping columns we dont need to make it easier to follow the process."
      ],
      "metadata": {
        "id": "6-PxMEhHq5YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_keep = ['OBJEKTNR', 'KLASS', 'geometry']\n",
        "wetlands = wetlands[columns_to_keep] # Drop columns not in columns_to_keep\n",
        "wetlands\n",
        "# class_names = ['1. very high nature valye', '2. high nature value', '3. some nature value', '4. low nature value']"
      ],
      "metadata": {
        "id": "P6VX7hguq-7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersect the ditch lines with the wetland polygons and calculate the lenght of ditch channels within each wetland."
      ],
      "metadata": {
        "id": "flA2O613AhEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intersect = gpd.overlay(ditches, wetlands, how='intersection')\n",
        "intersect['ditch_length'] = intersect.geometry.length # Calculate length of intersecting ditch lines\n",
        "wetland_ditch_length = intersect.groupby('OBJEKTNR')['ditch_length'].sum().reset_index() # Aggregate lengths by wetland polygon\n",
        "wetlands_with_ditch_length = wetlands.merge(wetland_ditch_length, on='OBJEKTNR', how='left').fillna({'ditch_length': 0})\n",
        "wetlands_with_ditch_length"
      ],
      "metadata": {
        "id": "M9lMww3U8dH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersect the wetland waterbodies polygons with the wetland polygons and calculate the waterbody area within each wetland."
      ],
      "metadata": {
        "id": "VmAUrZrkI3_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "intersect = gpd.overlay(waterbodies, wetlands, how='intersection')\n",
        "intersect['water_area'] = intersect.geometry.area # Calculate water area for intersecting waterbodies\n",
        "area_waterbodies = intersect.groupby('OBJEKTNR')['water_area'].sum().reset_index()\n",
        "wetlands_with_waterbodies = wetlands.merge(area_waterbodies, on='OBJEKTNR', how='left').fillna({'water_area': 0})\n",
        "wetlands_with_waterbodies # notice the new column \"water_area\""
      ],
      "metadata": {
        "id": "EbzVQ1CVyAFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For lakes and valuable forests we need to calculate the distance to nearest lake instead of intersecting wetlands and lakes.\n",
        "\n",
        "Start with lakes"
      ],
      "metadata": {
        "id": "pbXlRgB13ENL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from scipy.spatial import cKDTree\n",
        "\n",
        "wetlands_with_lake_distance = wetlands.copy()\n",
        "lake_spatial_index = cKDTree(lakes.centroid.apply(lambda x: (x.x, x.y)).tolist()) # Create a spatial index for the lake centroids\n",
        "\n",
        "# Function to calculate the nearest distance\n",
        "def nearest_distance(point):\n",
        "    distance, idx = lake_spatial_index.query((point.x, point.y))\n",
        "    return distance\n",
        "\n",
        "wetlands_with_lake_distance['nearest_lake_distance'] = wetlands_with_lake_distance.centroid.apply(nearest_distance) # Apply the function to each wetland centroid to get the nearest distance to a lake\n",
        "wetlands_with_lake_distance"
      ],
      "metadata": {
        "id": "adNILAIWxHtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then do the same with valuable forests"
      ],
      "metadata": {
        "id": "IrYX6k89lrtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from scipy.spatial import cKDTree\n",
        "\n",
        "wetlands_with_forest_distance = wetlands.copy()\n",
        "forest_spatial_index = cKDTree(forest.centroid.apply(lambda x: (x.x, x.y)).tolist()) # Create a spatial index for the lake centroids\n",
        "\n",
        "# Function to calculate the nearest distance\n",
        "def nearest_forest_distance(point):\n",
        "    distance, idx = forest_spatial_index.query((point.x, point.y))\n",
        "    return distance\n",
        "\n",
        "wetlands_with_forest_distance['nearest_forest_distance'] = wetlands_with_forest_distance.centroid.apply(nearest_forest_distance) # Apply the function to each wetland centroid to get the nearest distance to a lake\n",
        "wetlands_with_forest_distance"
      ],
      "metadata": {
        "id": "KoMcpDxY9ZjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally you need to join all geodataframes. The data can be joined based on the attribute 'OBJEKTNR'. This should give you a dataframe with ditch lenght, area of lakes within each wetland, distance to nearest lake, distance to nearest valuable forest."
      ],
      "metadata": {
        "id": "cmwu3Mx_nlwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "attribute_dataframes = [wetlands_with_ditch_length, wetlands_with_waterbodies, wetlands_with_lake_distance, wetlands_with_forest_distance]\n",
        "merged_wetlands = wetlands.copy()\n",
        "\n",
        "# Merge the dataframes one by one based on the ID of each wetland\n",
        "for df in attribute_dataframes:\n",
        "    cols_to_use = df.columns.difference(merged_wetlands.columns)\n",
        "    merged_wetlands = pd.merge(merged_wetlands, df[cols_to_use], left_index=True, right_index=True, how='outer')\n",
        "merged_wetlands"
      ],
      "metadata": {
        "id": "kOQcvSOl3I89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to drop the column for the ID and geometry so the machine learning model does not attempt to train on those attributes."
      ],
      "metadata": {
        "id": "sHMRle9vATLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data = merged_wetlands.drop(['geometry', 'OBJEKTNR'], axis=1)\n",
        "clean_data = clean_data.reset_index(drop=True)\n",
        "clean_data"
      ],
      "metadata": {
        "id": "puHS6b5WAa3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning with Scikit-Learn\n",
        "Now when we have a dataframe with wetlands and some attributes we can use them to train a model that can predict the nature value on mires. We will use [scikit-learn](https://scikit-learn.org/stable/) to build and test a basic random forest model. To evaluate weather the model is good or not we will split the data into training 80% and testing 20%.\n",
        "\n",
        "I highly encourage you to take some time to learn more about scikit-learn if you are interested in working with machine learning in the future. Here is a longer video if you want to learn more: [Long form video by Vincent](https://www.youtube.com/watch?v=0B5eIE_1vpU)"
      ],
      "metadata": {
        "id": "7zkrlxRQoW2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = clean_data.iloc[:,0] # This is Nature value KLASS\n",
        "x = clean_data.iloc[:,1:] # These are all the other attributes variables\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0, stratify = y) # splits the data into training and testing data. test_size=0.2 means that 20% of the wetlands will be set aside for testing.\n"
      ],
      "metadata": {
        "id": "UKINCm_r5XLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree\n",
        "It is always good to start with a simple model so we will build a [decision tree](https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using or training data and test it on our test data."
      ],
      "metadata": {
        "id": "I3pLL7SVGuJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(max_depth=3) # This number determains how many decisions the tree will contain\n",
        "clf.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "t3NsXiFLG6eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also inspect the trained decision tree to see whats going on."
      ],
      "metadata": {
        "id": "S2eLc-vDH2CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plot_tree(\n",
        "    clf,\n",
        "    feature_names=x.columns,\n",
        "    class_names=[str(c) for c in clf.classes_],  # convert to strings\n",
        "    filled=True,\n",
        "    fontsize=8\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xNDjqKc9HyAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first decision is on the lenght of ditch channels on a wetland.\n",
        "\n",
        "The standard way to evaluate a machine learning model is to use it to predict the test data and then compare the prediction to the test labels."
      ],
      "metadata": {
        "id": "7qGB8ElBHRgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = clf.predict(x_test) # clf is the name of our model defined above and .predict means that we use the model in prediction mode.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "IY8QIRTRHUiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is between 0 and 1 where 1 is 100% and something like 0.1 would be 10% accurate. In other words, the model would predict the right nature value class 10% of the time. For more detailed information we can use a classification report to see how the model preforms on different classes. The classification report produces values for precision, recall, f1-score and support. Lets break those down a bit. using class 1 (Mkt högt natuvärde/very high nature value).\n",
        "\n",
        "\n",
        "*   Precision = % of how many of the predicted wetlands in class 1 that were actually in class 1.\n",
        "*   Recall = % of all wetlands in class 1 that the model predicted as class 1.\n",
        "*   f1-score = A combination of precision and recall. If your model has high precision but low recall, or vice versa, the F1-score will be lower. Higher number is better.\n",
        "*   support = How many samples were in that class in the test data.\n",
        "\n",
        "\n",
        "This might be alot to take in so focus on the f1-score for now. we want as high f1-score as possible. You can also run the prediction on the same data that it was trained on to compare that with the prediction on the test data. If the difference is big then your model has overfitted to the training data.\n"
      ],
      "metadata": {
        "id": "u3-71L8hHk3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model on its own training data\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = clf.predict(x_train)\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "orNh2Qwg7DKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_test = clf.predict(x_test)\n",
        "print(classification_report(y_test, y_pred_test, zero_division=0))"
      ],
      "metadata": {
        "id": "MFwCx6Qn0HBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model seems to be best at classifying wetlands with low values \"Lågt naturvärde\".\n",
        "\n",
        "Another way to dive deeper into the model output is to look at a confusion matrix where each prediction is compared to the actual class of that wetland. Each square will show the number of correct predictions.\n"
      ],
      "metadata": {
        "id": "0jaakk4F16Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "# Get the class names\n",
        "class_names = ['1. very high nature valye', '2. high nature value', '3. some nature value', '4. low nature value']   # Replace with your class names\n",
        "\n",
        "# Visualize the confusion matrix with class names\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tz6va6bU0MqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IXpFvqUhWE8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision forest\n",
        "Now when you have seen a decision tree it's time to build a decision forest. It is the same thing but with more trees working together. The parameter in the code below \"n_estimators=2\" determains the number of trees in the forest. In this case it will train an ensabmle of 2 trees."
      ],
      "metadata": {
        "id": "Av5xc538C2Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=200, max_depth=3) # create a random forest model\n",
        "rf_clf.fit(x_train, y_train) # train the model"
      ],
      "metadata": {
        "id": "5tNOrhZWC-yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf_clf.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "_pqRV1p3DLgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also inspect the decision trees in a random forest but if the forest contains alot of trees this can be a bit tedious."
      ],
      "metadata": {
        "id": "-ff6YgysNe0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "from IPython.display import Image\n",
        "\n",
        "tree_index = 0  # Change this to select a different tree. Inspect the second tree in the forest by changing 0 to 1.\n",
        "tree = rf_clf.estimators_[tree_index]\n",
        "\n",
        "dot_data = export_graphviz(tree, out_file=None,\n",
        "                           feature_names=x.columns,\n",
        "                           class_names=[str(c) for c in clf.classes_],  # convert to strings\n",
        "                           filled=True, rounded=True)\n",
        "\n",
        "# Create a Graphviz object\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "RJAIlWxbNeSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With random forest we can also get an idea of what attributes are important for the model using feature importance. Keep in mind that machine learning is a bit of a black box so in order to see why a feature is important you would have to inspect all the decision trees in a random forest model."
      ],
      "metadata": {
        "id": "X5eCusrD453o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "feature_importance = rf_clf.feature_importances_\n",
        "feature_names = x.columns\n",
        "indices = feature_importance.argsort()[::-1]\n",
        "\n",
        "# create a dataframe for nicer plot\n",
        "importance_df = pd.DataFrame({'Feature': feature_names[indices],\n",
        "                              'Importance Score': feature_importance[indices]})\n",
        "\n",
        "# higher values are more important for the model\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance Score', y='Feature', data=importance_df, hue='Feature', palette='viridis', legend=False)\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "keErHczg5HTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3\n",
        "Train a random forest model with more trees and see if that improves the result. Can you think of a reason to not build as many trees as possible?\n"
      ],
      "metadata": {
        "id": "MELVt3iNMyP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement your model\n",
        "You have now trained a model and evaluated the preformance of the model. The next step is to combine the prediction with the original dataframe. In this case the geodataframe with all the combined data \"merged_wetlands\". Note that we want to keep the geometry and ID so we can save the result as a geopackage or shapefile."
      ],
      "metadata": {
        "id": "kCIR7yp86QM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "subset_df = merged_wetlands[x_train.columns] # Subset the DataFrame to include only the columns used for training\n",
        "\n",
        "predictions = rf_clf.predict(subset_df) # run the prediction\n",
        "predictions_df = pd.DataFrame(predictions, columns=['Prediction']) # Convert predictions to a DataFrame\n",
        "\n",
        "final_df = pd.concat([merged_wetlands, predictions_df], axis=1) # merge predictions with the original geodataframe\n",
        "final_df.to_file('/content/predicted_wetlands.gpkg', driver='GPKG')"
      ],
      "metadata": {
        "id": "QIQYUc-08cLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ejPlf1CrU70f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning on raster data\n",
        "Start by installing geopandas and cloning the course github repositroy where some example data is stored. The raster data will be stored under /content/Analyses-of-Environmental-Data-2/data/rasters and the field plots will be stored as csv files under under /content/Analyses-of-Environmental-Data-2/data/\n"
      ],
      "metadata": {
        "id": "YmITriRfBBn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas\n",
        "!git clone https://github.com/williamlidberg/Analyses-of-Environmental-Data-2.git"
      ],
      "metadata": {
        "id": "NN5pB4_2BUv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import and inspect the field data csv"
      ],
      "metadata": {
        "id": "0sF38y1av2Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "soildata = pd.read_csv('/content/Analyses-of-Environmental-Data-2/data/Krycklan_Soilsurvey_data.csv', sep=';')\n",
        "soildata"
      ],
      "metadata": {
        "id": "EVTPFVMDxB25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the data"
      ],
      "metadata": {
        "id": "RO_NdbHRxODT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5etYJErt9bMe"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "soildata_gdf = gpd.GeoDataFrame(soildata, geometry=gpd.points_from_xy(soildata.East, soildata.North), crs=3006)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,20)\n",
        "soildata_gdf.plot(column='SMC', cmap='viridis_r', legend=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rasterdata\n",
        "\n",
        "The following raster layers were calculated from the digital elevation model using whitebox tools. Don't panic, we will not use all of them.\n",
        "\n",
        "1.   DownslopeIndex_2m\n",
        "2.   DownslopeIndex_4m\n",
        "3.   DepthToWater_1ha\n",
        "4.   DepthToWater_2ha\n",
        "5.   DepthToWater_4ha\n",
        "6.   DepthToWater_8ha\n",
        "7.   DepthToWater_16ha\n",
        "8.   DepthToWater_32ha\n",
        "9.   ElevationAboveStream_1ha\n",
        "10.  ElevationAboveStream_2ha\n",
        "11.  ElevationAboveStream_4ha\n",
        "12.  ElevationAboveStream_8ha\n",
        "13.  ElevationAboveStream_16ha\n",
        "14.  ElevationAboveStream_32ha\n",
        "15.  PennocLandformClassification\n",
        "16.  PlanCurvature\n",
        "17.  RelativeTopographicPosition\n",
        "18.  TopographicWetnessIndex\n",
        "19.  WILT\n",
        "20.  DEM\n",
        "21.  Slope\n",
        "22.  DInfFlowaccumulation\n",
        "\n",
        "You need to extract the pixel values to the field plots. This can be done using a combination of [rasterio](https://rasterio.readthedocs.io/en/latest/) and [geopandas](https://geopandas.org/en/stable/). rasterio is a python package that focuses on reading and writing raster data. Start by installing it in your environment."
      ],
      "metadata": {
        "id": "fojAezvhxmez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "OfeayqXgv5VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we export anything its a good habit to inspect some of the data to make sure it looks like expected."
      ],
      "metadata": {
        "id": "mtMkVKfq1TGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "dem = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif')\n",
        "show(dem, cmap='viridis_r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yiFZq0331dcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1\n",
        "Plot some of the raster files under /content/Analyses-of-Environmental-Data-2/data/rasters/ so you get a sense of what the data is representing.\n",
        "\n",
        "Note that some muppet has mixed lower case and upper case letters in the names and python is case sensetive. slope and Slope are not the same."
      ],
      "metadata": {
        "id": "ZmB2pIr91ut3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract raster values to field plots\n",
        "If the data looks to be in order we can use raster io to extract the raster values to our field plots. This code first finds the x and y coordinates of each field plot in the geodataframe. \"coords = [(x,y) for x, y in zip(soildata_gdf.geometry.x, soildata_gdf.geometry.y)]\" it then loops over each field plot and extracts the raster values. Finally it adds the extracted values to a new column in the geodataframe. \"soildata_gdf['dem'] = [x[0] for x in src.sample(coords)]\""
      ],
      "metadata": {
        "id": "wbP_oHkV1rbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "import geopandas as gpd\n",
        "\n",
        "\n",
        "coords = [(x,y) for x, y in zip(soildata_gdf.geometry.x, soildata_gdf.geometry.y)]\n",
        "\n",
        "# Open the raster using rasterio and extract the pixel values to the geodataframe\n",
        "# dem\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif')\n",
        "soildata_gdf['dem'] = [x[0] for x in src.sample(coords)] # Naming is important to keep things in order\n",
        "# Slope\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/Slope/16m.tif')\n",
        "soildata_gdf['Slope'] = [x[0] for x in src.sample(coords)]\n",
        "# PlanCurvature\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/PlanCurvature/16m.tif')\n",
        "soildata_gdf['PlanCurvature'] = [x[0] for x in src.sample(coords)]\n",
        "# RelativeTopographicPosition\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/RelativeTopographicPosition/16m.tif')\n",
        "soildata_gdf['RelativeTopographicPosition'] = [x[0] for x in src.sample(coords)]\n",
        "# TopographicWetnessIndex\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/TopographicWetnessIndex/16m.tif')\n",
        "soildata_gdf['TopographicWetnessIndex'] = [x[0] for x in src.sample(coords)]\n",
        "# DownslopeIndex_2m\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/DownslopeIndex_2m/16m.tif')\n",
        "soildata_gdf['DownslopeIndex_2m'] = [x[0] for x in src.sample(coords)]\n",
        "# DepthToWater_1ha\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/DepthToWater_1ha/16m.tif')\n",
        "soildata_gdf['DepthToWater_1ha'] = [x[0] for x in src.sample(coords)]\n",
        "# DepthToWater_8ha\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/DepthToWater_8ha/16m.tif')\n",
        "soildata_gdf['DepthToWater_8ha'] = [x[0] for x in src.sample(coords)]\n"
      ],
      "metadata": {
        "id": "CMozRvMXwqsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now you have a new geodataframe with both the field data and the raster data"
      ],
      "metadata": {
        "id": "gpLjmCQzz0l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soildata_gdf"
      ],
      "metadata": {
        "id": "M5bnDEvUyrPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be useful to plot one of the variables to see if it makes any sense. compare this plot to the raster plots you did above."
      ],
      "metadata": {
        "id": "fbM2CvmV0q4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,20)\n",
        "soildata_gdf.plot(column='dem', cmap='viridis_r')"
      ],
      "metadata": {
        "id": "VZi6udFHz8O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are only interested in soil moisture now we will drop the other Y-variables. We also need to split the data into training data and testing data. The model will be trained on the training data and evaluated on the test data just like in module 7."
      ],
      "metadata": {
        "id": "QlhxV18D4Weo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soildata_clean = soildata_gdf.drop(soildata_gdf.columns[2:10], axis=1) # drops column 3 to 10\n",
        "soildata_clean = soildata_clean.drop(soildata_gdf.columns[0], axis=1) # drops column 0 which is the text for the soil moisture\n",
        "soildata_clean # SMC_code \t= Soil moisture code"
      ],
      "metadata": {
        "id": "koWTdsAW4vP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into training and testing\n",
        "Here we will use stratified sampling which means that sklearn will include examples of all classes in both the training data and the testing data."
      ],
      "metadata": {
        "id": "7xszut6E7m_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = soildata_clean.iloc[:,0] # This is soil moisture\n",
        "x = soildata_clean.iloc[:,1:] # These are all the topographical variables\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0, stratify = y)"
      ],
      "metadata": {
        "id": "6b29sSrd4Tn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning the hyperparameters\n",
        "In the field of machine learning things are named differntly than in traditional statistics. In statistics the settings for a model is sometimes refered to as features. However, in machine learning the features are the data you extracted to the points and the setting of the model is instead called hyperparameters. Much cooler. It is quite common to fiddle with these hyper paramters to see what works and this process can be autmated. This is known as tuning the hyperparameters.\n",
        "\n",
        "Here is an example using a tune grid where multiple models will be trained using all possible combinations of the settings listed bellow. This is a brute force approach and very demanding of your hardware. But computer time is cheaper than human time so lets do it."
      ],
      "metadata": {
        "id": "h8es3eq4yyy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "model = RandomForestClassifier() # note that we are using classification for the soil moisture classes\n",
        "\n",
        "\n",
        "tune_grid = {'n_estimators': [50, 100, 500],\n",
        "               'max_features': ['sqrt'],\n",
        "               'max_depth': [4,5,6],\n",
        "               'min_samples_split': [2, 5, 10],\n",
        "               'min_samples_leaf': [1, 2, 4],\n",
        "               'bootstrap': [True]}\n",
        "\n",
        "rf_random = RandomizedSearchCV(estimator = model, param_distributions = tune_grid, random_state=0, n_jobs = -1)\n",
        "\n",
        "# Train the model using the optimal hyperparameters\n",
        "rf_random.fit(x_train, y_train)\n",
        "\n",
        "print('The best combination of hyperparameters was', rf_random.best_params_)"
      ],
      "metadata": {
        "id": "03uB9Zdu72Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect important features"
      ],
      "metadata": {
        "id": "j8tUBGmLRMtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "best_rf = rf_random.best_estimator_\n",
        "\n",
        "# Extract feature importances\n",
        "importances = best_rf.feature_importances_\n",
        "\n",
        "# Sort from most important to least important\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "feature_names = list(x_train.columns)\n",
        "importances = best_rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.barh(np.array(feature_names)[indices], importances[indices])\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.gca().invert_yaxis()   # largest at top\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yC9ehzkXM8wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model just like in module 7. Note that the accuracy is between 0 and 1 so 0.5 is 50%."
      ],
      "metadata": {
        "id": "g3HTnLAo0B3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = rf_random.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "QsLXsJCd8T2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the F1-score for each soil moisture class and pay attention to the support column which shows how many plots are within each category. It's harder to learn with few examples from a class. Remember that the soil moisture classes were\n",
        "\n",
        "\n",
        "*   1 = Dry\n",
        "*   2 = Mesic\n",
        "*   3 = Mesic-moist\n",
        "*   4 = Moist\n",
        "*   2 = Wet"
      ],
      "metadata": {
        "id": "0JQtfUDr0Qad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_test = rf_random.predict(x_test)\n",
        "print(classification_report(y_test, y_pred_test, zero_division=0))"
      ],
      "metadata": {
        "id": "VoI1fiFX8IbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement a machine learning model on raster data\n",
        "Now we have a working model that we want to apply to the Krycklan catchment. We need to read all the rasterlayers, apply the model and then save the result as a new raster.\n",
        "\n",
        "All rasterdata will be read into numpy arrays using gdal."
      ],
      "metadata": {
        "id": "aWrmvVB28kcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from osgeo import gdal_array\n",
        "import numpy as np\n",
        "# Read raster data as numeric array from file\n",
        "dem = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif')\n",
        "Slope = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/Slope/16m.tif')\n",
        "PlanCurvature = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/PlanCurvature/16m.tif')\n",
        "RelativeTopographicPosition = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/RelativeTopographicPosition/16m.tif')\n",
        "TopographicWetnessIndex = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/TopographicWetnessIndex/16m.tif')\n",
        "DownslopeIndex_2m = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/DownslopeIndex_2m/16m.tif')\n",
        "DepthToWater_1ha = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/DepthToWater_1ha/16m.tif')\n",
        "DepthToWater_8ha = gdal_array.LoadFile('/content/Analyses-of-Environmental-Data-2/data/rasters/DepthToWater_8ha/16m.tif')\n"
      ],
      "metadata": {
        "id": "o7Fsllox8nQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a list of all arrays you wish to include. Note that you need to add or remove the variables in both the list and the converted dataframe."
      ],
      "metadata": {
        "id": "yp1tb_PW9kkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of all arrays. you can\n",
        "list_or_all_rasters = [dem, Slope, PlanCurvature, RelativeTopographicPosition, TopographicWetnessIndex, DownslopeIndex_2m, DepthToWater_1ha, DepthToWater_8ha]\n",
        "\n",
        "all_data = np.array(list_or_all_rasters)\n",
        "all_data=all_data.reshape(8,738*662).T # The shape is from the original DEM\n",
        "\n",
        "df_data=pd.DataFrame(all_data,columns=['dem', 'Slope', 'PlanCurvature', 'RelativeTopographicPosition', 'TopographicWetnessIndex', 'DownslopeIndex_2m', 'DepthToWater_1ha', 'DepthToWater_8ha'])\n"
      ],
      "metadata": {
        "id": "QYR4m7pJ9i6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = rf_random.predict(df_data)\n",
        "\n",
        "# Save the data as a raster file with coordinates and extent from one of the input layers\n",
        "result = result.reshape(738,662)\n",
        "extent = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/dem/16m.tif')\n",
        "\n",
        "with rasterio.Env():\n",
        "  profile = extent.profile\n",
        "  with rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/prediction.tif', 'w', **profile) as dst:\n",
        "        dst.write(result, 1)"
      ],
      "metadata": {
        "id": "cl1vzwYc-Hpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also plot the result for a quick inspection"
      ],
      "metadata": {
        "id": "XRnLpyeP-88I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from matplotlib import pyplot as plt\n",
        "src = rasterio.open('/content/Analyses-of-Environmental-Data-2/data/rasters/prediction.tif')\n",
        "plt.imshow(src.read(1), cmap='viridis_r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_T4mZ_yV-LFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmaleQy2OXGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}